#install.packages("tidymodels")
library(tidymodels)
#install.packages("tidyverse")
library(tidyverse)
#install.packages("skimr")
library(skimr)
#install.packages("corrplot")
library(corrplot)
#install.packages("GGally")
library(GGally)
#install.packages("themis")
library(themis)
#install.packages("rpart.plot")
library(rpart.plot)
#install.packages("xgboost")
library(xgboost)
#install.packages("lubridate")
library(lubridate)
#install.packages("ggridges")
library(ggridges)
#install.packages("DescTools")
library(DescTools)
#install.packages("NbClust")
library(NbClust)
#install.packages("ggdendro")
library(ggdendro)
#install.packages("klaR")
library(klaR)
#install.packages("glmnet")
library(glmnet)
#install.packages("doParallel")
library(doParallel)
#install.packages("ranger")
library(ranger)
# load the censored dataset and helper functions
load("./offers_censored.RData")
source("./helpful_functions.R")
# display the first few rows of the censored dataset
head(offers)
# define a function to summarize the contents of a dataset
years_and_max_dates <- function(x) {
x |>
group_by(AppYear) |>
summarise(
`Num observations` = n(),
`Max \`AppDate\`` = max(AppDate),
`Max \`OfferDate\`` = max(OfferDate),
`Max \`ResponseDate\`` = max(ResponseDate, na.rm = TRUE),
`\`ResponseDate\` is NA` = sum(is.na(ResponseDate))
)
}
# set the seed number so that the results are reproducible
set.seed(420)
# define a function to make the final training and prediction split
final_training_prediction_split <-
offers |>
make_appyear_split(test_year = 2023)
# extract the final training dataset
offers_final_training <- training(final_training_prediction_split)
# extract the prediction dataset
offers_prediction <- testing(final_training_prediction_split)
# inspect the final_training set
offers_final_training |> years_and_max_dates()
# inspect the prediction set
offers_prediction |> years_and_max_dates()
# set the seed number so that the results are reproducible
set.seed(420)
# define a function to make the analysis and assessment split
analysis_assessment_split <-
offers |>
filter(AppYear <= 2022) |>
censor_post_prediction_responses(years = 2022) |>
drop_post_prediction_offers(years = 2022) |>
make_appyear_split(test_year = 2022)
# extract the analysis dataset
offers_analysis <- training(analysis_assessment_split)
# extract the assessment dataset
offers_assessment <- testing(analysis_assessment_split)
# inspect the analysis set
offers_analysis |> years_and_max_dates()
# inspect the assessment set
offers_assessment |> years_and_max_dates()
skim(offers_analysis)
offers_analysis |>
group_by(Status) |>
count()
# Status and Response
ggplot(offers_analysis, aes(x = Response, fill = Status)) +
geom_bar(position = "dodge", stat = "count") +
labs(title = "Distribution of Status per Response Category",
x = "Response",
y = "Count")
# Status and Demo1
ggplot(offers_analysis, aes(x = Demo1, fill = Status)) +
geom_bar(position = "dodge", stat = "count") +
labs(title = "Distribution of Status per Demo1 Category",
x = "Demo1",
y = "Count")
# Status and Demo2
ggplot(offers_analysis, aes(x = Demo2, fill = Status)) +
geom_bar(position = "dodge", stat = "count") +
labs(title = "Distribution of Status per Demo2 Category",
x = "Demo2",
y = "Count")
# Status and App2
ggplot(offers_analysis, aes(x = App2, fill = Status)) +
geom_bar(position = "fill", stat = "count") +
labs(x = "App2",
y = "Proportion")
# set the seed number so that the results are reproducible
set.seed(420)
# 10-folds cross-validation
cv_folds <- vfold_cv(offers_analysis, v = 10, strata = "Status")
# specify the metrics of interest
class_metrics <- metric_set(f_meas, roc_auc, sensitivity, specificity)
# define the recipe
lasso_recipe <-
recipe(Status ~ ., data = offers_analysis) |>
# remove the variables that will not be used
update_role(AppYear, AppDate, OfferDate, ResponseDate, new_role = "metadata") |>
# convert all nominal variables to dummy variables
step_dummy(all_nominal_predictors()) |>
step_zv(all_predictors()) |>
# normalize the variables to have mean 0 and standard deviation 1
step_normalize(all_predictors()) |>
# downsample to balance the classes in the dependent variable
step_downsample(Status)
lasso_model <-
logistic_reg(penalty = tune(), mixture = 1) |>
set_engine("glmnet")
lasso_workflow <-
workflow() |>
add_recipe(lasso_recipe) |>
add_model(lasso_model)
grid_lasso <- grid_regular(penalty(c(-3, -1), trans = log10_trans()),
levels = 30)
grid_lasso
# set the seed number so that the results are reproducible
set.seed(420)
lasso_tune <-
lasso_workflow |>
tune_grid(
resamples = cv_folds,
grid = grid_lasso,
metrics = class_metrics)
lasso_tune |>
autoplot() +
theme_bw()
# collect the metrics
lasso_tune_metrics <-
lasso_tune |>
collect_metrics()
# plot the f_meas
lasso_tune_metrics |>
filter(.metric == "f_meas") |>
ggplot(aes(
x = penalty, y = mean,
ymin = mean - std_err, ymax = mean + std_err
)) +
geom_pointrange(alpha = 0.5, size = .125) +
scale_x_log10() +
labs(y = "f_meas", x = expression(lambda)) +
theme_bw()
lasso_tune |>
show_best(metric = "f_meas")
lasso_1se_model <-
lasso_tune |>
select_by_one_std_err(metric = "f_meas", desc(penalty))
lasso_1se_model
lasso_workflow_tuned <-
lasso_workflow |>
finalize_workflow(lasso_1se_model)
lasso_workflow_tuned
# fit the model to the entire analysis set and evaluate the model on the assessment set
lasso_last_fit <-
lasso_workflow_tuned |>
last_fit(analysis_assessment_split, metrics = class_metrics)
# collect the metrics on the assessment set
lasso_test_metrics <-
lasso_last_fit |>
collect_metrics()
lasso_test_metrics
rf_recipe <-
recipe(Status ~ ., data = offers_analysis) |>
update_role(AppDate, OfferDate, AppYear, ResponseDate, HowFirstHeard, new_role = "metadata")
rf_model_tune <-
rand_forest(mtry = tune(), trees = 50) |>
set_mode("classification") |>
set_engine("ranger", importance = "permutation")
rf_tune_wf <-
workflow() |>
add_recipe(rf_recipe) |>
add_model(rf_model_tune)
rf_tune_grid <- grid_regular(mtry(range = c(1, 12)), levels = 12)
rf_tune_grid
num_cores <- parallel::detectCores()
num_cores
doParallel::registerDoParallel(cores = num_cores - 1L)
set.seed(420)
rf_tune_res <- tune_grid(
rf_tune_wf,
resamples = cv_folds,
grid = rf_tune_grid,
metrics = class_metrics)
# by f1 score
rf_tune_res |>
show_best(metric = "f_meas", n = 5) |>
arrange(desc(mtry))
# by sensitivity
rf_tune_res |>
show_best(metric = "sensitivity", n = 5) |>
arrange(desc(mtry))
# by specificity
rf_tune_res |>
show_best(metric = "specificity", n = 5) |>
arrange(desc(mtry))
rf_tune_res |>
collect_metrics() |>
filter(.metric %in% c("sensitivity", "specificity")) |>
ggplot(aes(
x = mtry, y = mean, ymin = mean - std_err,
ymax = mean + std_err,
colour = .metric
)) +
geom_errorbar() +
geom_line() +
geom_point() +
scale_colour_manual(values = c("#D55E00", "#0072B2")) +
facet_wrap(~.metric, ncol = 1, scales = "free_y") +
guides(colour = "none") +
theme_bw()
rf_tune_res |>
collect_metrics() |>
filter(.metric %in% c("specificity", "f_meas", "sensitivity")) |>
ggplot(aes(
x = mtry, y = mean, ymin = mean - std_err,
ymax = mean + std_err, colour = .metric
)) +
geom_errorbar() +
geom_line() +
geom_point() +
scale_colour_manual(values = c("#D55E00", "#0072B2", "#009E73")) +
facet_wrap(~.metric, ncol = 1, scales = "free_y") +
guides(colour = "none") +
theme_bw()
rf_tune_res |>
collect_metrics() |>
filter(.metric %in% c("specificity", "f_meas", "sensitivity")) |>
ggplot(aes(
x = mtry, y = mean, ymin = mean - std_err,
ymax = mean + std_err, colour = .metric
)) +
geom_errorbar() +
geom_line() +
geom_point() +
scale_colour_manual(values = c("#D55E00", "#0072B2", "#009E73")) +
facet_wrap(~.metric, ncol = 1, scales = "free_y") +
guides(colour = "none") +
theme_bw()
best_rf <- select_best(rf_tune_res, metric = "f_meas")
rf_workflow_tuned <-
rf_tune_wf |>
finalize_workflow(best_rf)
rf_workflow_tuned
# fit the model to the entire analysis set and evaluate the model on the assessment set
rf_final_fit <-
rf_workflow_tuned |>
last_fit(analysis_assessment_split, metrics = class_metrics)
# collect the metrics on the assessment set
rf_test_results <-
rf_final_fit |>
collect_metrics()
rf_test_results
lasso_test_metrics
rf_test_results
metric <- metric_set(roc_auc)
final_model <-
lasso_workflow_tuned |>
last_fit(final_training_prediction_split, metrics = metric)
final_model |>
augment() |>
group_by(Program) |>
summarise(
Predicted_N = sum(.pred_Enrolled >= .5),
Predicted_Prob = mean(.pred_Enrolled)
)
# load the uncensored dataset
load("./offers_uncensored.RData")
set.seed(420)
final_training_prediction_split <-
offers |>
make_appyear_split(test_year = 2023)
training(final_training_prediction_split) |> years_and_max_dates()
testing(final_training_prediction_split) |> years_and_max_dates()
offers_final_training <- training(final_training_prediction_split)
offers_prediction <- testing(final_training_prediction_split)
final_model_uncensored <-
lasso_workflow_tuned |>
last_fit(final_training_prediction_split, metrics = class_metrics)
final_model_uncensored_aug <-
final_model_uncensored |>
augment()
final_model_uncensored_aug
# find the F1_score
primary_metric <- metric_set(f_meas)
final_model_uncensored_aug |>
primary_metric(truth = Status, estimate = .pred_class)
# create the confusion matrix (threshold 0.5)
final_model_uncensored_aug |>
conf_mat(truth = Status, estimate = .pred_class)
# create the confusion matrix (threshold 0.65)
final_model_uncensored_aug |>
mutate(.pred_class_65 = factor(ifelse(.pred_Enrolled >= 0.65, "Enrolled", "Not enrolled"))) |>
conf_mat(truth = Status, estimate = .pred_class_65)
